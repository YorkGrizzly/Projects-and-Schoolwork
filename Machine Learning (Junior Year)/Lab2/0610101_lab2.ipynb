{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression\n",
    "### 2. KNN\n",
    "### 3. Support Vector Machine\n",
    "\n",
    "unbalanced training data:\n",
    "(1) x171\n",
    "(0) x77 \n",
    "\n",
    "use confusion matrix \\\n",
    "(https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28)\n",
    "\n",
    "4. compare model performance (compare the average accuracy of cross validation on different models and see who has highest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method:\n",
    "# PREPROCESS\n",
    "# 1. read csv file\n",
    "# 2. normalize data (delete unimportant features) [standardize, binning]\n",
    "# 3. split data into train and val set\n",
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool functions\n",
    "\n",
    "\n",
    "def accuracy(y_pred, Y):\n",
    "    correct = 0\n",
    "    for i in range(len(Y)):\n",
    "        if y_pred[i] == Y[i]: correct += 1\n",
    "    accuracy = float(correct) / len(Y)\n",
    "    return accuracy\n",
    "\n",
    "def normalize(X):\n",
    "    return (X - X.min(0)) / X.ptp(0)\n",
    "    \n",
    "def standardize(X): # standardize x_data\n",
    "    mean_vector = np.mean(X, axis=0)\n",
    "    std_vector = np.std(X, axis=0)\n",
    "    return (X - mean_vector) / std_vector\n",
    "\n",
    "def remove_outliers(X, Y, threshold=3): # identify and remove outliers\n",
    "    #identify outliers\n",
    "    z = np.abs(standardize(X))\n",
    "\n",
    "    outliers = np.where(z > threshold)\n",
    "\n",
    "    #remove outlying rows\n",
    "    outlying_rows = [row for row in outliers[0]]\n",
    "    X = np.delete(X, outlying_rows, axis=0)\n",
    "    Y = np.delete(Y, outlying_rows, axis=0)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data # label array of data\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.x_data, self.y_data\n",
    "    \n",
    "    def concat_labels(self): # concatenate labels to data\n",
    "        self.y_data = self.y_data[np.newaxis, :] # convert label array to 2d\n",
    "        dataset = np.concatenate((self.x_data, self.y_data.T), axis=1) # append labels to data\n",
    "        return dataset\n",
    "        \n",
    "    def peel_labels(self, dataset): # peel labels from data\n",
    "        y_data = dataset[: , -1:]\n",
    "        y_data = y_data.flatten() # turn back into 1D array\n",
    "        x_data = dataset[: , :-1]\n",
    "        return x_data, y_data\n",
    "        \n",
    "    def shuffle_data(self): # shuffles dataset x and y\n",
    "        dataset = self.concat_labels()\n",
    "        np.random.shuffle(dataset) # in-place shuffle dataset\n",
    "        self.x_data, self.y_data = self.peel_labels(dataset)\n",
    "        self.y_data = self.y_data.astype(int)\n",
    "    \n",
    "    def remove_outliers(self):\n",
    "        pass\n",
    "        \n",
    "    def train_test_split(self, test_size=0.25, shuffle=True):\n",
    "        \n",
    "        test_len = int(test_size*len(self.x_data))\n",
    "        \n",
    "        if shuffle == True:\n",
    "            self.shuffle_data()\n",
    "            \n",
    "        x_train = self.x_data[:test_len]\n",
    "        y_train = self.y_data[:test_len]\n",
    "        x_test = self.x_data[test_len:]\n",
    "        y_test =  self.y_data[test_len:]\n",
    "        \n",
    "        return (x_train, y_train), (x_test, y_test)\n",
    "    \n",
    "    def cv_fold_split(self, folds=10, shuffle=True): # folds=0 indicates doing LOOCV (leave-one-out cross validation)\n",
    "        \n",
    "        if shuffle == True:\n",
    "            self.shuffle_data()\n",
    "        \n",
    "        x_fold_set = []\n",
    "        y_fold_set = []\n",
    "        \n",
    "        if folds == 0:\n",
    "            folds = len(self.x_data) # doing LOOCV\n",
    "        \n",
    "        fold_len = int(len(self.x_data) / folds) # the size per fold\n",
    "            \n",
    "        for i in range(folds): # create fold set with folds of data\n",
    "            x_fold_set.append(self.x_data[(i*fold_len):(i+1)*fold_len])\n",
    "            y_fold_set.append(self.y_data[(i*fold_len):(i+1)*fold_len])\n",
    "        \n",
    "        return x_fold_set, y_fold_set # list with nparrays\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validation\n",
    "1. shuffle data randomly\n",
    "2. split the dataset into k groups\n",
    "3. for each unique group:\\\n",
    "    i. take the group as a test data set\\\n",
    "    ii. take the remaining group as a training data set\\\n",
    "    iii. fit a model on the training set and evaluate it on the test set\\\n",
    "    iv. retain the evaluation score and discard the model\n",
    "4. summarize the skill of the model using the sample of  model evaluation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "\n",
    "class KNearestNeighbors:\n",
    "    \n",
    "    def __init__(self, k_neighbors=5):\n",
    "        self.k_neighbors = k_neighbors\n",
    "    \n",
    "    def euclidean_dist(self, pointA, pointB):\n",
    "        distance = np.linalg.norm(pointA - pointB)\n",
    "        return distance\n",
    "    \n",
    "    def predict(self, base_dataset, base_datalabel, X):\n",
    "        y_pred = []\n",
    "        for target_x in X:\n",
    "            distance_set = []\n",
    "            for index, datapoint in enumerate(base_dataset):\n",
    "                dist = self.euclidean_dist(datapoint, target_x)\n",
    "                distance_set.append((dist, index))\n",
    "            distance_set.sort() # by default, sort() sorts by first item in tuple (sort() is in place, does not return value)\n",
    "            nearest_neighbors = distance_set[:self.k_neighbors]\n",
    "\n",
    "            neighbor_labels = []\n",
    "            for neighbor in nearest_neighbors:\n",
    "                neighbor_labels.append(base_datalabel[neighbor[1]]) # get label of neighbor\n",
    "            try:\n",
    "                y_pred.append(mode(neighbor_labels)) # for python versions before 3.8, statistics.mode() raises error when more than one common\n",
    "            except:\n",
    "                import random\n",
    "                y_pred.append(random.randrange(0,2)) # generate random 0 or 1\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, max_iter=100):\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1 + np.exp(-z))\n",
    "    \n",
    "    \n",
    "    def cost_func(self, y_hat, y):\n",
    "        loss = - (y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)).mean()        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def fit(self, X, Y, LEARNING_RATE=0.001):\n",
    "        \n",
    "        n_features = len(X[0])\n",
    "        data_size = len(X)\n",
    "        \n",
    "        self.w = np.zeros(n_features)  # initialize weights\n",
    "        self.b = 0\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            y_hat = self.sigmoid(np.dot(X, self.w) + self.b)\n",
    "\n",
    "            loss = self.cost_func(y_hat, Y)\n",
    "#             print(\"loss: \" + str(loss))\n",
    "            \n",
    "            w_gradient = np.dot(X.T, (y_hat - Y)) / data_size # calculate gradients\n",
    "            \n",
    "            b_gradient = np.sum(y_hat - Y) / data_size\n",
    "            \n",
    "            self.w -= LEARNING_RATE * w_gradient # update weights\n",
    "            self.b -= LEARNING_RATE * b_gradient\n",
    "    \n",
    "    \n",
    "    def predict(self, X): # use the resulting y_hat array to make predicted array\n",
    "        y_hat = self.sigmoid(np.dot(X, self.w) + self.b)\n",
    "        y_pred = np.zeros(len(y_hat))\n",
    "        for i in range(len(y_hat)):\n",
    "            if y_hat[i] > 0.5:\n",
    "                y_pred[i] = 1\n",
    "            else:\n",
    "                y_pred[i] = 0\n",
    "        return y_pred.astype(int)\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./nctu-ml-2020-lab2/X_train.csv\"\n",
    "x_train = pd.read_csv(path, usecols=[i for i in range(1,13)]).to_numpy()\n",
    "\n",
    "path = \"./nctu-ml-2020-lab2/y_train.csv\"\n",
    "y_train = pd.read_csv(path, usecols=[1]).to_numpy()\n",
    "y_train = y_train.flatten() # convert 2d array to 1d\n",
    "\n",
    "path = \"./nctu-ml-2020-lab2/X_test.csv\"\n",
    "x_test = pd.read_csv(path, usecols=[i for i in range(1,13)]).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalize or standardize data\n",
    "x_train = standardize(x_train)\n",
    "x_test = standardize(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot labels with respect to each feature\n",
    "from matplotlib import pyplot as plt\n",
    "for i in range(len(x_train[0])):\n",
    "    fig, ax = plt.subplots(figsize=(8,3))\n",
    "    ax.scatter(x_train[ : , i], y_train)\n",
    "    ax.set_xlabel('Feature ' + str(i+1) + ' of X')\n",
    "    ax.set_ylabel('Y label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "standardized train, test 500 iter, lr=0.1 ----> accuracy: 0.83720\\\n",
    "standardized train, test 500 iter, lr=0.1, remove outliers ---> accuracy: 0.79069\\\n",
    "standardized train, test 5000000 iter, lr=0.001, remove outliers ---> accuracy: 0.81395\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8916666666666667\n"
     ]
    }
   ],
   "source": [
    "# preprocess = Preprocess(x_train, y_train)\n",
    "# (x_train, y_train), (x_test, y_test) = preprocess.train_test_split()\n",
    "# x_train, y_train = preprocess.get_data()\n",
    "\n",
    "x_train, y_train = remove_outliers(x_train, y_train)\n",
    "x_train = standardize(x_train)\n",
    "x_test = standardize(x_test)\n",
    "# 5000000, 0.0001\n",
    "LR = LogisticRegression(max_iter=5000000)\n",
    "LR.fit(x_train, y_train, LEARNING_RATE=0.001)\n",
    "y_train_pred = LR.predict(x_train)\n",
    "print(accuracy(y_train_pred, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_test_pred = LR.predict(x_test)\n",
    "accuracy(y_test_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame(data=y_pred, index=[i for i in range(len(y_pred))], columns=[\"Class\"])\n",
    "y_pred.index.name = \"ID\"\n",
    "y_pred.to_csv(\"0610101_result.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train = standardize(x_train)\n",
    "preprocess = Preprocess(x_train, y_train)\n",
    "\n",
    "x_fold_set, y_fold_set = preprocess.cv_fold_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg_train_accuracy = 0\n",
    "avg_valid_accuracy = 0\n",
    "for i in range(len(x_fold_set)):\n",
    "    x_train_set = list(x_fold_set) # copy list (since list.copy() doesn't work in old versions)\n",
    "    x_valid_set = x_train_set.pop(i)\n",
    "    x_train_set = np.concatenate(x_train_set, axis=0)\n",
    "\n",
    "    y_train_set = list(y_fold_set)\n",
    "    y_valid_set = y_train_set.pop(i)\n",
    "    y_train_set = np.concatenate(y_train_set, axis=0)\n",
    "\n",
    "    LR = LogisticRegression(max_iter=5000)\n",
    "    LR.fit(x_train_set, y_train_set, LEARNING_RATE=0.1)\n",
    "    y_train_pred = LR.predict(x_train_set)\n",
    "    y_valid_pred = LR.predict(x_valid_set)\n",
    "    \n",
    "#     print(\"using \" + str(i) + \" as validation set:\")\n",
    "    print(\"training accuracy ====> \" + str(accuracy(y_train_pred, y_train_set)))\n",
    "    avg_train_accuracy += accuracy(y_train_pred, y_train_set)\n",
    "    print(\"validation accuracy ===> \" + str(accuracy(y_valid_pred, y_valid_set)))\n",
    "    avg_valid_accuracy += accuracy(y_valid_pred, y_valid_set)\n",
    "    \n",
    "print(\"average training accuracy: \" + str(avg_train_accuracy / 10))\n",
    "print(\"average validation accuracy: \" + str(avg_valid_accuracy / 10))\n",
    "\n",
    "#NORMALIZE THE TESTING SET TOO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Techniques:\\\n",
    "    normalization yields highest accuracy on testing set (0.7674)\\\n",
    "    no preprocessing (0.69767)\\\n",
    "    standardization (0.62790)\\\n",
    "    \\\n",
    "    remove outliers:(k=10)\\\n",
    "        no preprocessing (0.69767)\\\n",
    "        normalization (0.62790)\\\n",
    "        standardized (0.62790)\n",
    "        \n",
    "    remove outliers:(k=18)\\\n",
    "        none (0.76744)\n",
    "        standardized (0.72093)\\\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove outliers\n",
    "x_train, y_train = remove_outliers(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train = standardize(x_train)\n",
    "x_test = standardize(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN = KNearestNeighbors(k_neighbors=18)\n",
    "y_pred = KNN.predict(x_train, y_train, x_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred = pd.DataFrame(data=y_pred, index=[i for i in range(len(y_pred))], columns=[\"Class\"])\n",
    "y_pred.index.name = \"ID\"\n",
    "y_pred.to_csv(\"0610101_result.csv\", sep=',')"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit47634c11a1d2441ab5e7322abc67a0a7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
